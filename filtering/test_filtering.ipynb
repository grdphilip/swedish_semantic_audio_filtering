{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "from transformers import WhisperFeatureExtractor\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "from model import MusCALL\n",
    "from dataset import AudioCaptionDataset\n",
    "import librosa\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cosine_similarity(audio_features, text_features):\n",
    "    cosine_sim = F.cosine_similarity(text_features, audio_features, dim=-1)\n",
    "    cosine_sim_bounded = (1 + cosine_sim) / 2\n",
    "    return cosine_sim_bounded\n",
    "\n",
    "def load_embeddings():\n",
    "    # Check if the embeddings files exist\n",
    "    if not os.path.exists(\"embeddings/synthetic_audio_features.pkl\") or not os.path.exists(\"embeddings/synthetic_text_features.pkl\"):\n",
    "        print(\"Embeddings files not found. Please run the extract_embeddings() method first.\")\n",
    "        return\n",
    "\n",
    "    audio_features_path = \"embeddings/\" + \"synthetic_audio_features.pkl\"\n",
    "    text_features_path = \"embeddings/\" + \"synthetic_text_features.pkl\"\n",
    "\n",
    "    with open(audio_features_path, 'rb') as af_file:\n",
    "        audio_features = pickle.load(af_file)\n",
    "\n",
    "    with open(text_features_path, 'rb') as tf_file:\n",
    "        text_features = pickle.load(tf_file)\n",
    "\n",
    "    return audio_features, text_features\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FilteringFramework:\n",
    "    def __init__(self, config, pretrained_model_path):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.device = torch.device(self.config.training.device)\n",
    "        self.feature_extractor = WhisperFeatureExtractor.from_pretrained(\"KBLab/kb-whisper-medium\")\n",
    "        self.checkpoint_path = pretrained_model_path\n",
    "\n",
    "        self.path_to_model = os.path.join(\n",
    "            self.config.env.experiments_dir,\n",
    "            self.config.env.experiment_id,\n",
    "            \"best_model.pth.tar\",\n",
    "        )\n",
    "                \n",
    "        self.set_seed()\n",
    "        self.load_dataset()\n",
    "        self.load_model()\n",
    "        self.similarities = None\n",
    "\n",
    "    def collate_fn(self, batch):\n",
    "        input_audio, text_input_ids, text_attention_mask, idx = zip(*batch)\n",
    "        \n",
    "        original_mel_spectograms = self.feature_extractor(input_audio, sampling_rate=16000, max_length=480000, return_tensors=\"pt\").input_features\n",
    "\n",
    "        text_input_ids = torch.stack(text_input_ids)\n",
    "        text_attention_mask = torch.stack(text_attention_mask)\n",
    "\n",
    "        max_len = max([len(i) for i in input_audio])\n",
    "\n",
    "        original_audio = []\n",
    "        for audio in input_audio:\n",
    "            if len(audio) < max_len:\n",
    "                zeros_needed = np.zeros(max_len - len(audio))\n",
    "                audio = np.concatenate((audio, zeros_needed), axis=0)\n",
    "                original_audio.append(audio)\n",
    "            else:    \n",
    "                original_audio.append(audio)\n",
    "\n",
    "        original_audio = np.stack(original_audio)\n",
    "\n",
    "        return {\"input_audio\": original_mel_spectograms.to(self.device), \\\n",
    "                \"original_audio\": original_audio, \\\n",
    "                \"text_input_ids\": text_input_ids.to(self.device), \\\n",
    "                \"text_attention_mask\": text_attention_mask.to(self.device),\\\n",
    "                \"idx\": idx}\n",
    "    \n",
    "    def load_dataset(self):\n",
    "        dataset = AudioCaptionDataset(self.config.dataset_config, dataset_type=\"to_filter\")\n",
    "        self.batch_size = 16\n",
    "        self.data_loader = DataLoader(\n",
    "            dataset=dataset,\n",
    "            batch_size=self.batch_size,\n",
    "            drop_last=False,\n",
    "            collate_fn=self.collate_fn,\n",
    "        )\n",
    "\n",
    "    def load_model(self):\n",
    "        self.model = MusCALL(self.config.model_config)\n",
    "        self.model.load_state_dict(torch.load(self.checkpoint_path), strict=False)\n",
    "        self.model.to(self.device)\n",
    "        self.model.eval()\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "    def set_seed(self,seed=42):\n",
    "        torch.manual_seed(seed)\n",
    "        np.random.seed(seed)\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "\n",
    "\n",
    "\n",
    "    def extract_embeddings(self):\n",
    "        # Create a directory to save the features called \"embeddings\"\n",
    "        if not os.path.exists(\"embeddings\"):\n",
    "            os.makedirs(\"embeddings\")\n",
    "\n",
    "        audio_features_path = \"embeddings/\" + \"synthetic_audio_features.pkl\"\n",
    "\n",
    "        text_features_path = \"embeddings/\" + \"synthetic_text_features.pkl\"\n",
    "        \n",
    "        dataset_size = len(self.data_loader.dataset)\n",
    "\n",
    "        all_audio_features = torch.zeros(dataset_size, 512, device=self.device)\n",
    "        all_text_features = torch.zeros(dataset_size, 512, device=self.device)\n",
    "\n",
    "        total_samples_processed = 0\n",
    "        \n",
    "        for batch in tqdm(self.data_loader, desc=\"Loading data\", leave=False):\n",
    "            original_mel_spectograms = batch[\"input_audio\"].to(self.device)\n",
    "            text_input_ids = batch[\"text_input_ids\"].to(self.device)\n",
    "            text_attention_mask = batch[\"text_attention_mask\"].to(self.device)\n",
    "\n",
    "            \n",
    "            audio_features = self.model.encode_audio(original_mel_spectograms)\n",
    "            text_features = self.model.encode_text(text_input_ids, text_attention_mask)\n",
    "\n",
    "            batch_size = audio_features.size(0)\n",
    "\n",
    "            all_audio_features[total_samples_processed:total_samples_processed + batch_size] = audio_features\n",
    "            all_text_features[total_samples_processed:total_samples_processed + batch_size] = text_features\n",
    "\n",
    "            total_samples_processed += batch_size\n",
    "\n",
    "        # Convert tensors to CPU before saving to avoid GPU-related issues in the pickle files\n",
    "        audio_features = all_audio_features.cpu()\n",
    "        text_features = all_text_features.cpu()\n",
    "\n",
    "        # Save the features to pickle files\n",
    "        with open(audio_features_path, 'wb') as af_file:\n",
    "            pickle.dump(audio_features, af_file)\n",
    "\n",
    "        with open(text_features_path, 'wb') as tf_file:\n",
    "            pickle.dump(text_features, tf_file)\n",
    "\n",
    "        return audio_features, text_features\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    def apply_filtering(self, synthetic_data_manifest, stdev_threshold):\n",
    "        \n",
    "        mean = np.mean(self.similarities)\n",
    "        stdev = np.std(self.similarities)\n",
    "\n",
    "        # Identify the condition for outliers\n",
    "        condition = self.similarities < (mean - stdev_threshold * stdev)\n",
    "\n",
    "        # Get the indices of the outliers\n",
    "        outlier_indices = np.where(condition)[0]\n",
    "\n",
    "        # Get the samples to delete\n",
    "        samples_to_delete = []\n",
    "        audio_durations = 0\n",
    "        for i, sample in enumerate(synthetic_data_manifest):\n",
    "            if i in outlier_indices:\n",
    "                samples_to_delete.append(sample['audio_id'])    \n",
    "                audio_path = sample['audio_path']\n",
    "                audio, sr = librosa.load(audio_path, sr=None)  # Load the audio file\n",
    "                duration = librosa.get_duration(y=audio, sr=sr)  # Get the duration in seconds\n",
    "                audio_durations += duration\n",
    "\n",
    "        # Convert to minutes\n",
    "        audio_durations = audio_durations / 60\n",
    "\n",
    "        print(f\"Total number of samples to delete: {len(samples_to_delete)}\")\n",
    "        print(f\"Total audio duration to delete: {audio_durations} minutes\")\n",
    "\n",
    "        return samples_to_delete\n",
    "    \n",
    "        \n",
    "    def get_similarities(self, audio_features, text_features):        \n",
    "        similarities = []\n",
    "        for embedding_audio, embedding_text in zip(audio_features, text_features):\n",
    "            similarities.append(compute_cosine_similarity(embedding_text.unsqueeze(0), embedding_audio.unsqueeze(0)))\n",
    "        \n",
    "        similarities_tensor = torch.tensor(similarities)\n",
    "        self.similarities = similarities_tensor.numpy()\n",
    "        \n",
    "        \n",
    "        # Save the updated data manifest\n",
    "\n",
    "\n",
    "    def run(self, data_manifest_path, stdev_threshold=3):\n",
    "        # Extract embeddings\n",
    "        audio_features, text_features = self.extract_embeddings()\n",
    "        print(f\"Embeddings extracted. {audio_features.shape}, {text_features.shape}\")\n",
    "\n",
    "        # Compute similarities\n",
    "        self.get_similarities(audio_features, text_features)\n",
    "        sim_copy = self.similarities.copy()\n",
    "        print(f\"Similarities computed. {len(sim_copy)}\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from filtering import FilteringFramework\n",
    "from model_utils import merge_conf\n",
    "import argparse\n",
    "\n",
    "def main():\n",
    "    data_manifest_path = \"data/common_voice_16_0_train_manifest.json\"\n",
    "    base_conf_path = 'configs/base_config.yaml'\n",
    "    dataset_conf_path = 'configs/dataset.yaml'\n",
    "    model_conf_path = 'configs/model.yaml'\n",
    "    config = merge_conf(base_conf_path, dataset_conf_path, model_conf_path)\n",
    "\n",
    "    filtering = FilteringFramework(config, pretrained_model_path='save/experiments/model1/checkpoint.pth.tar')\n",
    "\n",
    "    filtering.run(data_manifest_path=args.data_manifest_path)\n",
    "\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"data_manifest_path\", type=str, default='data/common_voice_16_0_train_manifest.json')\n",
    "args = parser.parse_args()\n",
    "\n",
    "main(args)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "save_dir = os.path.dirname(data_manifest_path)\n",
    "save_path = os.path.join(save_dir, \"dist_fb.png\")\n",
    "print(self.similarities)\n",
    "\n",
    "# Plot the distribution of similarity values\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.hist(self.similarities, bins=30, color='skyblue', edgecolor='black', alpha=0.7)\n",
    "plt.title('Distribution of Similarity Values', fontsize=16)\n",
    "plt.xlabel('Similarity', fontsize=14)\n",
    "plt.ylabel('Frequency', fontsize=14)\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "plt.xticks(fontsize=12)\n",
    "plt.yticks(fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.savefig(save_path)\n",
    "plt.close()\n",
    "\n",
    "audio_features = audio_features.detach().cpu().numpy()\n",
    "text_features = text_features.detach().cpu().numpy()\n",
    "\n",
    "# Determine the correct perplexity value\n",
    "n_samples = min(len(audio_features), len(text_features))\n",
    "perplexity_value = min(30, n_samples - 1)  # Ensure valid perplexity\n",
    "\n",
    "if n_samples < 2:\n",
    "    print(\"Not enough samples for t-SNE visualization.\")\n",
    "    raise ValueError\n",
    "\n",
    "# Apply t-SNE\n",
    "tsne = TSNE(n_components=2, perplexity=perplexity_value, random_state=42)\n",
    "audio_2d = tsne.fit_transform(audio_features)\n",
    "text_2d = tsne.fit_transform(text_features)\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(text_2d[:, 0], text_2d[:, 1], c='blue', label=\"Text Embeddings\", alpha=0.6, s=10)\n",
    "plt.scatter(audio_2d[:, 0], audio_2d[:, 1], c='red', label=\"Audio Embeddings\", alpha=0.6, s=10)\n",
    "plt.legend()\n",
    "plt.title(\"t-SNE Visualization of Text & Audio Embeddings\")\n",
    "\n",
    "# Save to the same directory as `data_manifest_path`\n",
    "\n",
    "save_path = os.path.join(save_dir, \"tsne_plot.png\")\n",
    "plt.savefig(save_path)\n",
    "print(f\"t-SNE plot saved to: {save_path}\")\n",
    "\n",
    "plt.close()\n",
    "\n",
    "        \n",
    "        \n",
    "        #self.apply_filtering(data_manifest_path, stdev_threshold)\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
