{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!bzip2 -d svt-2023.xml.bz2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: spacy 3.8.4\n",
      "Uninstalling spacy-3.8.4:\n",
      "  Would remove:\n",
      "    /Library/Frameworks/Python.framework/Versions/3.10/bin/spacy\n",
      "    /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/spacy-3.8.4.dist-info/*\n",
      "    /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/spacy/*\n",
      "Proceed (Y/n)? ^C\n"
     ]
    }
   ],
   "source": [
    "!pip3.10 uninstall spacy\n",
    "# grep -o \"<sentence\" svt-2023.xml | wc -l\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m xml_file \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msvt-2023.xml\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      5\u001b[0m count \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m event, elem \u001b[38;5;129;01min\u001b[39;00m ET\u001b[38;5;241m.\u001b[39miterparse(xml_file, events\u001b[38;5;241m=\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mend\u001b[39m\u001b[38;5;124m\"\u001b[39m,)):\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m elem\u001b[38;5;241m.\u001b[39mtag \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msentence\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m      8\u001b[0m         count \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/xml/etree/ElementTree.py:1259\u001b[0m, in \u001b[0;36miterparse.<locals>.iterator\u001b[0;34m()\u001b[0m\n\u001b[1;32m   1257\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m data:\n\u001b[1;32m   1258\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m-> 1259\u001b[0m     \u001b[43mpullparser\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1260\u001b[0m root \u001b[38;5;241m=\u001b[39m pullparser\u001b[38;5;241m.\u001b[39m_close_and_return_root()\n\u001b[1;32m   1261\u001b[0m \u001b[38;5;28;01myield from\u001b[39;00m pullparser\u001b[38;5;241m.\u001b[39mread_events()\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/xml/etree/ElementTree.py:1301\u001b[0m, in \u001b[0;36mXMLPullParser.feed\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m   1299\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m data:\n\u001b[1;32m   1300\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1301\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_parser\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1302\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mSyntaxError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m   1303\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_events_queue\u001b[38;5;241m.\u001b[39mappend(exc)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "xml_file = \"svt-2023.xml\"\n",
    "\n",
    "count = 0\n",
    "for event, elem in ET.iterparse(xml_file, events=(\"end\",)):\n",
    "    if elem.tag == \"sentence\":\n",
    "        count += 1\n",
    "        elem.clear()  # Free memory\n",
    "\n",
    "print(f\"Total sentences in the file: {count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at KBLab/bert-base-swedish-cased-ner were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Additional names found: []\n",
      "Additional names found: []\n",
      "Found possible name: Jwan\n",
      "Found possible name: Ali\n",
      "Additional names found: ['Jwan', 'Ali']\n",
      "Found possible name: Det\n",
      "Additional names found: ['Det']\n",
      "Found possible name: Jacob\n",
      "Found possible name: Lagercrantz\n",
      "Additional names found: ['Jacob', 'Lagercrantz']\n",
      "Additional names found: []\n",
      "Found possible name: Det\n",
      "Additional names found: ['Det']\n",
      "Additional names found: []\n",
      "Found possible name: Fredriksson\n",
      "Additional names found: ['Fredriksson']\n",
      "Found possible name: Där\n",
      "Found possible name: Henrik\n",
      "Found possible name: Johansson\n",
      "Additional names found: ['Där', 'Henrik', 'Johansson']\n",
      "Found possible name: Polenfärjan\n",
      "Found possible name: Stor\n",
      "Additional names found: ['Polenfärjan', 'Stor']\n",
      "Found possible name: Karlshamnspiloten\n",
      "Additional names found: ['Karlshamnspiloten']\n",
      "Additional names found: []\n",
      "Additional names found: []\n",
      "Additional names found: []\n",
      "Found possible name: Det\n",
      "Additional names found: ['Det']\n",
      "Additional names found: []\n",
      "Additional names found: []\n",
      "Additional names found: []\n",
      "Found possible name: Bowin\n",
      "Additional names found: ['Bowin']\n",
      "Found possible name: Rassol\n",
      "Found possible name: Blekingeföreningar\n",
      "Additional names found: ['Rassol', 'Blekingeföreningar']\n",
      "Additional names found: []\n",
      "Additional names found: []\n",
      "Found possible name: Bergström\n",
      "Additional names found: ['Bergström']\n",
      "Found possible name: Sammansvetsad\n",
      "Additional names found: ['Sammansvetsad']\n",
      "Additional names found: []\n",
      "Found possible name: Fotboll\n",
      "Additional names found: ['Fotboll']\n",
      "Additional names found: []\n",
      "Found possible name: Klemerstam\n",
      "Additional names found: ['Klemerstam']\n",
      "Found possible name: Det\n",
      "Found possible name: Magnus\n",
      "Found possible name: Ljungcrantz\n",
      "Additional names found: ['Det', 'Magnus', 'Ljungcrantz']\n",
      "Found possible name: Nyheter\n",
      "Found possible name: Blekinge\n",
      "Additional names found: ['Nyheter', 'Blekinge']\n",
      "Additional names found: []\n",
      "Additional names found: []\n",
      "Additional names found: []\n",
      "Additional names found: []\n",
      "Additional names found: []\n",
      "Additional names found: []\n",
      "Found possible name: Nästa\n",
      "Found possible name: Fredrik\n",
      "Found possible name: Svennergren\n",
      "Additional names found: ['Nästa', 'Fredrik', 'Svennergren']\n",
      "Additional names found: []\n",
      "Found possible name: Hör\n",
      "Additional names found: ['Hör']\n",
      "Found possible name: Twitterkonto\n",
      "Found possible name: Pelle\n",
      "Additional names found: ['Twitterkonto', 'Pelle']\n",
      "Additional names found: []\n",
      "Additional names found: []\n",
      "Additional names found: []\n",
      "Additional names found: []\n",
      "Additional names found: []\n",
      "Additional names found: []\n",
      "Found possible name: Inge\n",
      "Found possible name: Persson\n",
      "Additional names found: ['Inge', 'Persson']\n",
      "Found possible name: Sturesson\n",
      "Additional names found: ['Sturesson']\n",
      "Found possible name: Just\n",
      "Additional names found: ['Just']\n",
      "✅ Processed first 50 sentences with BERT! Saved as sentences_entities_bert.csv.\n"
     ]
    }
   ],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "import csv\n",
    "from transformers import pipeline\n",
    "import re\n",
    "\n",
    "# Load Swedish BERT NER model\n",
    "nlp = pipeline(\"ner\", model=\"KBLab/bert-base-swedish-cased-ner\", tokenizer=\"KBLab/bert-base-swedish-cased-ner\")\n",
    "\n",
    "# Define input/output files\n",
    "xml_file = \"svt-2023.xml\"\n",
    "csv_file = \"sentences_entities_bert.csv\"\n",
    "\n",
    "# Helper function to extract capitalized words as potential names\n",
    "def extract_capitalized_words(sentence, found_entities):\n",
    "    words = sentence.split()\n",
    "    possible_names = []\n",
    "    for i, word in enumerate(words):\n",
    "        # If the word starts with an uppercase letter and is not the first word (to avoid sentence-start capitalization)\n",
    "        if re.match(r\"^[A-ZÅÄÖ][a-zåäö]+$\", word) and i > 0:\n",
    "            if word not in found_entities and len(word) > 3:\n",
    "                print(f\"Found possible name: {word}\")\n",
    "                possible_names.append(word)\n",
    "    return possible_names\n",
    "\n",
    "def clean_sentence(sentence):\n",
    "    # Remove unwanted symbols but keep .,!? (normal punctuation)\n",
    "    sentence = re.sub(r'[-:…]', ' ', sentence)  # Replace dashes and colons with space\n",
    "    sentence = re.sub(r'\\[.*?\\]', '', sentence)  # Remove text inside [...]\n",
    "    \n",
    "    # Normalize spaces\n",
    "    sentence = re.sub(r'\\s+', ' ', sentence).strip()\n",
    "    return sentence\n",
    "\n",
    "# Open CSV file for writing\n",
    "with open(csv_file, \"w\", encoding=\"utf-8\", newline=\"\") as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow([\"sentence\", \"entities\"])  # CSV headers\n",
    "\n",
    "    # Parse XML\n",
    "    context = ET.iterparse(xml_file, events=(\"start\", \"end\"))\n",
    "    count = 0\n",
    "\n",
    "    for event, elem in context:\n",
    "        if event == \"end\" and elem.tag == \"sentence\":\n",
    "            # Extract sentence text\n",
    "            words = [token.text for token in elem.findall(\".//token\") if token.text]\n",
    "            sentence = \" \".join(words)\n",
    "            \n",
    "            \n",
    "\n",
    "            # Run BERT NER model\n",
    "            bert_entities = []\n",
    "            current_entity = \"\"\n",
    "            current_label = None\n",
    "\n",
    "            for token in nlp(sentence):\n",
    "                word = token[\"word\"]\n",
    "                label = token[\"entity\"]\n",
    "                score = token[\"score\"]\n",
    "\n",
    "                if word.startswith(\"##\"):  \n",
    "                    # Append subword parts correctly\n",
    "                    current_entity += word[2:]\n",
    "                else:\n",
    "                    if current_entity and current_label in [\"PER\", \"ORG\", \"LOC\", \"EVN\"]:\n",
    "                        bert_entities.append(current_entity)  # Store completed entity\n",
    "\n",
    "                    # Start a new entity only if confidence is high\n",
    "                    if score > 0.95 and label in [\"PER\", \"ORG\", \"LOC\", \"EVN\"]:\n",
    "                        current_entity = word\n",
    "                        current_label = label\n",
    "                    else:\n",
    "                        current_entity = \"\"\n",
    "                        current_label = None\n",
    "\n",
    "            if current_entity and current_label in [\"PER\", \"ORG\", \"LOC\", \"EVN\"]:\n",
    "                bert_entities.append(current_entity)  # Add the last entity\n",
    "\n",
    "            # Write to CSV\n",
    "            sentence = clean_sentence(sentence)\n",
    "            writer.writerow([sentence, \", \".join(all_entities)])\n",
    "\n",
    "            # Free memory\n",
    "            elem.clear()\n",
    "\n",
    "            count += 1\n",
    "            if count >= 50:  # Limit to 50 sentences for testing\n",
    "                break\n",
    "\n",
    "print(f\"✅ Processed first 50 sentences with BERT! Saved as {csv_file}.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at KBLab/bert-base-swedish-cased-ner were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'entity': 'TME', 'score': 0.99950325, 'index': 1, 'word': 'Under', 'start': 0, 'end': 5}\n",
      "{'entity': 'TME', 'score': 0.9993074, 'index': 2, 'word': 'lördags', 'start': 6, 'end': 13}\n",
      "{'entity': 'TME', 'score': 0.9996593, 'index': 3, 'word': '##för', 'start': 13, 'end': 16}\n",
      "{'entity': 'TME', 'score': 0.99969447, 'index': 4, 'word': '##middagen', 'start': 16, 'end': 24}\n",
      "{'entity': 'PER', 'score': 0.99985874, 'index': 7, 'word': 'J', 'start': 36, 'end': 37}\n",
      "{'entity': 'PER', 'score': 0.99984276, 'index': 8, 'word': '##wan', 'start': 37, 'end': 40}\n",
      "{'entity': 'PER', 'score': 0.9997129, 'index': 9, 'word': 'Ali', 'start': 41, 'end': 44}\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "nlp = pipeline(\"ner\", model=\"KBLab/bert-base-swedish-cased-ner\", tokenizer=\"KBLab/bert-base-swedish-cased-ner\")\n",
    "\n",
    "test_sentence = \"Under lördagsförmiddagen kom ägaren Jwan Ali till platsen .\"\n",
    "\n",
    "# Run NER\n",
    "tokens = nlp(test_sentence)\n",
    "\n",
    "# Print results\n",
    "for token in tokens:\n",
    "    print(token)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
